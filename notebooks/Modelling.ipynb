{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **<center>Modelling** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# Resolve an error that I had with Cuda\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "plt.style.use('dark_background')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all the columns in the .head() method\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the dotenv variables\n",
    "load_dotenv()\n",
    "\n",
    "# Get the the path variable from dotenv\n",
    "project_path = os.getenv('Project_Path')[2:78]\n",
    "\n",
    "# Change notebook directory back one so that it can acess the data\n",
    "os.chdir(project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/processed/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(255720, 62)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>term</th>\n",
       "      <th>grade</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>pymnt_plan</th>\n",
       "      <th>inq_last_6mths</th>\n",
       "      <th>pub_rec</th>\n",
       "      <th>revol_bal</th>\n",
       "      <th>revol_util</th>\n",
       "      <th>total_acc</th>\n",
       "      <th>initial_list_status</th>\n",
       "      <th>last_pymnt_amnt</th>\n",
       "      <th>last_credit_pull_d</th>\n",
       "      <th>collections_12_mths_ex_med</th>\n",
       "      <th>mths_since_last_major_derog</th>\n",
       "      <th>dti_joint</th>\n",
       "      <th>tot_cur_bal</th>\n",
       "      <th>open_acc_6m</th>\n",
       "      <th>il_util</th>\n",
       "      <th>max_bal_bc</th>\n",
       "      <th>inq_last_12m</th>\n",
       "      <th>emp_type_Accountant</th>\n",
       "      <th>emp_type_Admin</th>\n",
       "      <th>emp_type_Analyst</th>\n",
       "      <th>emp_type_Assistant</th>\n",
       "      <th>emp_type_Clergy</th>\n",
       "      <th>emp_type_Clerk</th>\n",
       "      <th>emp_type_Designer</th>\n",
       "      <th>emp_type_Director</th>\n",
       "      <th>emp_type_Education</th>\n",
       "      <th>emp_type_Executive</th>\n",
       "      <th>emp_type_Healer</th>\n",
       "      <th>emp_type_Manager</th>\n",
       "      <th>emp_type_Operator</th>\n",
       "      <th>emp_type_Technical</th>\n",
       "      <th>emp_type_Vol</th>\n",
       "      <th>home_ownership_OTHER</th>\n",
       "      <th>home_ownership_RENT</th>\n",
       "      <th>verification_status_Source Verified</th>\n",
       "      <th>verification_status_Verified</th>\n",
       "      <th>purpose_credit_card</th>\n",
       "      <th>purpose_major_purchase</th>\n",
       "      <th>purpose_other</th>\n",
       "      <th>purpose_small_business</th>\n",
       "      <th>purpose_wedding</th>\n",
       "      <th>addr_state_AL</th>\n",
       "      <th>addr_state_CO</th>\n",
       "      <th>addr_state_DC</th>\n",
       "      <th>addr_state_FL</th>\n",
       "      <th>addr_state_IL</th>\n",
       "      <th>addr_state_KS</th>\n",
       "      <th>addr_state_ME</th>\n",
       "      <th>addr_state_MS</th>\n",
       "      <th>addr_state_ND</th>\n",
       "      <th>addr_state_NE</th>\n",
       "      <th>addr_state_NH</th>\n",
       "      <th>addr_state_NV</th>\n",
       "      <th>addr_state_NY</th>\n",
       "      <th>addr_state_SC</th>\n",
       "      <th>addr_state_TX</th>\n",
       "      <th>addr_state_VA</th>\n",
       "      <th>frac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.156460</td>\n",
       "      <td>-0.654724</td>\n",
       "      <td>0.763889</td>\n",
       "      <td>-0.789014</td>\n",
       "      <td>-0.003357</td>\n",
       "      <td>0.305877</td>\n",
       "      <td>-0.335522</td>\n",
       "      <td>-0.145932</td>\n",
       "      <td>1.201140</td>\n",
       "      <td>-1.373775</td>\n",
       "      <td>-0.97077</td>\n",
       "      <td>-0.415561</td>\n",
       "      <td>0.302329</td>\n",
       "      <td>-0.107149</td>\n",
       "      <td>0.576897</td>\n",
       "      <td>-0.023957</td>\n",
       "      <td>-0.843347</td>\n",
       "      <td>-0.103891</td>\n",
       "      <td>-0.139209</td>\n",
       "      <td>-0.116273</td>\n",
       "      <td>-0.088306</td>\n",
       "      <td>-0.109205</td>\n",
       "      <td>-0.104843</td>\n",
       "      <td>-0.157505</td>\n",
       "      <td>-0.147428</td>\n",
       "      <td>-0.039637</td>\n",
       "      <td>-0.077757</td>\n",
       "      <td>-0.052789</td>\n",
       "      <td>-0.196448</td>\n",
       "      <td>-0.186479</td>\n",
       "      <td>-0.180873</td>\n",
       "      <td>-0.245189</td>\n",
       "      <td>-0.435078</td>\n",
       "      <td>-0.169413</td>\n",
       "      <td>-0.395006</td>\n",
       "      <td>-0.172517</td>\n",
       "      <td>-0.016276</td>\n",
       "      <td>1.2214</td>\n",
       "      <td>-0.768632</td>\n",
       "      <td>1.431317</td>\n",
       "      <td>1.817653</td>\n",
       "      <td>-0.140912</td>\n",
       "      <td>-0.225373</td>\n",
       "      <td>-0.108777</td>\n",
       "      <td>-0.051496</td>\n",
       "      <td>-0.113061</td>\n",
       "      <td>-0.147149</td>\n",
       "      <td>-0.052423</td>\n",
       "      <td>-0.271536</td>\n",
       "      <td>-0.204067</td>\n",
       "      <td>-0.094934</td>\n",
       "      <td>-0.024331</td>\n",
       "      <td>-0.065744</td>\n",
       "      <td>-0.02324</td>\n",
       "      <td>-0.036428</td>\n",
       "      <td>-0.069732</td>\n",
       "      <td>-0.119254</td>\n",
       "      <td>-0.301818</td>\n",
       "      <td>-0.110158</td>\n",
       "      <td>-0.295217</td>\n",
       "      <td>-0.174612</td>\n",
       "      <td>1.171905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.452829</td>\n",
       "      <td>1.527360</td>\n",
       "      <td>-0.314193</td>\n",
       "      <td>-0.696292</td>\n",
       "      <td>-0.003357</td>\n",
       "      <td>4.312132</td>\n",
       "      <td>-0.335522</td>\n",
       "      <td>-0.679268</td>\n",
       "      <td>-1.912396</td>\n",
       "      <td>-1.796028</td>\n",
       "      <td>-0.97077</td>\n",
       "      <td>-0.426398</td>\n",
       "      <td>-3.540893</td>\n",
       "      <td>-0.107149</td>\n",
       "      <td>0.576897</td>\n",
       "      <td>-0.023957</td>\n",
       "      <td>-0.843347</td>\n",
       "      <td>-0.103891</td>\n",
       "      <td>-0.139209</td>\n",
       "      <td>-0.116273</td>\n",
       "      <td>-0.088306</td>\n",
       "      <td>-0.109205</td>\n",
       "      <td>-0.104843</td>\n",
       "      <td>-0.157505</td>\n",
       "      <td>-0.147428</td>\n",
       "      <td>-0.039637</td>\n",
       "      <td>-0.077757</td>\n",
       "      <td>-0.052789</td>\n",
       "      <td>-0.196448</td>\n",
       "      <td>-0.186479</td>\n",
       "      <td>-0.180873</td>\n",
       "      <td>-0.245189</td>\n",
       "      <td>-0.435078</td>\n",
       "      <td>-0.169413</td>\n",
       "      <td>-0.395006</td>\n",
       "      <td>-0.172517</td>\n",
       "      <td>-0.016276</td>\n",
       "      <td>1.2214</td>\n",
       "      <td>1.301013</td>\n",
       "      <td>-0.698657</td>\n",
       "      <td>-0.550160</td>\n",
       "      <td>-0.140912</td>\n",
       "      <td>-0.225373</td>\n",
       "      <td>-0.108777</td>\n",
       "      <td>-0.051496</td>\n",
       "      <td>-0.113061</td>\n",
       "      <td>-0.147149</td>\n",
       "      <td>-0.052423</td>\n",
       "      <td>-0.271536</td>\n",
       "      <td>-0.204067</td>\n",
       "      <td>-0.094934</td>\n",
       "      <td>-0.024331</td>\n",
       "      <td>-0.065744</td>\n",
       "      <td>-0.02324</td>\n",
       "      <td>-0.036428</td>\n",
       "      <td>-0.069732</td>\n",
       "      <td>-0.119254</td>\n",
       "      <td>-0.301818</td>\n",
       "      <td>-0.110158</td>\n",
       "      <td>-0.295217</td>\n",
       "      <td>-0.174612</td>\n",
       "      <td>0.403643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.464683</td>\n",
       "      <td>-0.654724</td>\n",
       "      <td>-0.468204</td>\n",
       "      <td>-0.970563</td>\n",
       "      <td>-0.003357</td>\n",
       "      <td>1.307441</td>\n",
       "      <td>-0.335522</td>\n",
       "      <td>-0.622684</td>\n",
       "      <td>1.821333</td>\n",
       "      <td>-1.289324</td>\n",
       "      <td>-0.97077</td>\n",
       "      <td>-0.315809</td>\n",
       "      <td>0.302329</td>\n",
       "      <td>-0.107149</td>\n",
       "      <td>0.576897</td>\n",
       "      <td>-0.023957</td>\n",
       "      <td>-0.843347</td>\n",
       "      <td>-0.103891</td>\n",
       "      <td>-0.139209</td>\n",
       "      <td>-0.116273</td>\n",
       "      <td>-0.088306</td>\n",
       "      <td>-0.109205</td>\n",
       "      <td>-0.104843</td>\n",
       "      <td>-0.157505</td>\n",
       "      <td>-0.147428</td>\n",
       "      <td>-0.039637</td>\n",
       "      <td>-0.077757</td>\n",
       "      <td>-0.052789</td>\n",
       "      <td>-0.196448</td>\n",
       "      <td>-0.186479</td>\n",
       "      <td>-0.180873</td>\n",
       "      <td>-0.245189</td>\n",
       "      <td>-0.435078</td>\n",
       "      <td>-0.169413</td>\n",
       "      <td>-0.395006</td>\n",
       "      <td>-0.172517</td>\n",
       "      <td>-0.016276</td>\n",
       "      <td>1.2214</td>\n",
       "      <td>-0.768632</td>\n",
       "      <td>-0.698657</td>\n",
       "      <td>-0.550160</td>\n",
       "      <td>-0.140912</td>\n",
       "      <td>-0.225373</td>\n",
       "      <td>9.193151</td>\n",
       "      <td>-0.051496</td>\n",
       "      <td>-0.113061</td>\n",
       "      <td>-0.147149</td>\n",
       "      <td>-0.052423</td>\n",
       "      <td>-0.271536</td>\n",
       "      <td>4.900357</td>\n",
       "      <td>-0.094934</td>\n",
       "      <td>-0.024331</td>\n",
       "      <td>-0.065744</td>\n",
       "      <td>-0.02324</td>\n",
       "      <td>-0.036428</td>\n",
       "      <td>-0.069732</td>\n",
       "      <td>-0.119254</td>\n",
       "      <td>-0.301818</td>\n",
       "      <td>-0.110158</td>\n",
       "      <td>-0.295217</td>\n",
       "      <td>-0.174612</td>\n",
       "      <td>1.250750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.563724</td>\n",
       "      <td>-0.654724</td>\n",
       "      <td>0.147842</td>\n",
       "      <td>-0.399582</td>\n",
       "      <td>-0.003357</td>\n",
       "      <td>0.305877</td>\n",
       "      <td>-0.335522</td>\n",
       "      <td>-0.504878</td>\n",
       "      <td>-1.426299</td>\n",
       "      <td>0.990842</td>\n",
       "      <td>-0.97077</td>\n",
       "      <td>-0.376798</td>\n",
       "      <td>-1.344122</td>\n",
       "      <td>-0.107149</td>\n",
       "      <td>0.576897</td>\n",
       "      <td>-0.023957</td>\n",
       "      <td>-0.843347</td>\n",
       "      <td>-0.103891</td>\n",
       "      <td>-0.139209</td>\n",
       "      <td>-0.116273</td>\n",
       "      <td>-0.088306</td>\n",
       "      <td>-0.109205</td>\n",
       "      <td>-0.104843</td>\n",
       "      <td>-0.157505</td>\n",
       "      <td>-0.147428</td>\n",
       "      <td>-0.039637</td>\n",
       "      <td>-0.077757</td>\n",
       "      <td>-0.052789</td>\n",
       "      <td>-0.196448</td>\n",
       "      <td>-0.186479</td>\n",
       "      <td>-0.180873</td>\n",
       "      <td>-0.245189</td>\n",
       "      <td>-0.435078</td>\n",
       "      <td>-0.169413</td>\n",
       "      <td>-0.395006</td>\n",
       "      <td>-0.172517</td>\n",
       "      <td>-0.016276</td>\n",
       "      <td>1.2214</td>\n",
       "      <td>1.301013</td>\n",
       "      <td>-0.698657</td>\n",
       "      <td>-0.550160</td>\n",
       "      <td>-0.140912</td>\n",
       "      <td>4.437084</td>\n",
       "      <td>-0.108777</td>\n",
       "      <td>-0.051496</td>\n",
       "      <td>-0.113061</td>\n",
       "      <td>-0.147149</td>\n",
       "      <td>-0.052423</td>\n",
       "      <td>-0.271536</td>\n",
       "      <td>-0.204067</td>\n",
       "      <td>-0.094934</td>\n",
       "      <td>-0.024331</td>\n",
       "      <td>-0.065744</td>\n",
       "      <td>-0.02324</td>\n",
       "      <td>-0.036428</td>\n",
       "      <td>-0.069732</td>\n",
       "      <td>-0.119254</td>\n",
       "      <td>-0.301818</td>\n",
       "      <td>-0.110158</td>\n",
       "      <td>-0.295217</td>\n",
       "      <td>-0.174612</td>\n",
       "      <td>1.222458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.156460</td>\n",
       "      <td>-0.654724</td>\n",
       "      <td>1.225924</td>\n",
       "      <td>-0.603570</td>\n",
       "      <td>-0.003357</td>\n",
       "      <td>2.309004</td>\n",
       "      <td>-0.335522</td>\n",
       "      <td>-0.399424</td>\n",
       "      <td>-1.120393</td>\n",
       "      <td>-1.120423</td>\n",
       "      <td>-0.97077</td>\n",
       "      <td>-0.417770</td>\n",
       "      <td>-0.247992</td>\n",
       "      <td>-0.107149</td>\n",
       "      <td>0.576897</td>\n",
       "      <td>-0.023957</td>\n",
       "      <td>-0.843347</td>\n",
       "      <td>-0.103891</td>\n",
       "      <td>-0.139209</td>\n",
       "      <td>-0.116273</td>\n",
       "      <td>-0.088306</td>\n",
       "      <td>-0.109205</td>\n",
       "      <td>-0.104843</td>\n",
       "      <td>-0.157505</td>\n",
       "      <td>-0.147428</td>\n",
       "      <td>-0.039637</td>\n",
       "      <td>-0.077757</td>\n",
       "      <td>-0.052789</td>\n",
       "      <td>-0.196448</td>\n",
       "      <td>-0.186479</td>\n",
       "      <td>-0.180873</td>\n",
       "      <td>-0.245189</td>\n",
       "      <td>-0.435078</td>\n",
       "      <td>-0.169413</td>\n",
       "      <td>-0.395006</td>\n",
       "      <td>-0.172517</td>\n",
       "      <td>-0.016276</td>\n",
       "      <td>1.2214</td>\n",
       "      <td>1.301013</td>\n",
       "      <td>-0.698657</td>\n",
       "      <td>-0.550160</td>\n",
       "      <td>-0.140912</td>\n",
       "      <td>-0.225373</td>\n",
       "      <td>-0.108777</td>\n",
       "      <td>19.418825</td>\n",
       "      <td>-0.113061</td>\n",
       "      <td>-0.147149</td>\n",
       "      <td>-0.052423</td>\n",
       "      <td>-0.271536</td>\n",
       "      <td>-0.204067</td>\n",
       "      <td>-0.094934</td>\n",
       "      <td>-0.024331</td>\n",
       "      <td>-0.065744</td>\n",
       "      <td>-0.02324</td>\n",
       "      <td>-0.036428</td>\n",
       "      <td>-0.069732</td>\n",
       "      <td>-0.119254</td>\n",
       "      <td>-0.301818</td>\n",
       "      <td>-0.110158</td>\n",
       "      <td>-0.295217</td>\n",
       "      <td>-0.174612</td>\n",
       "      <td>1.125997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   loan_amnt      term     grade  annual_inc  pymnt_plan  inq_last_6mths  \\\n",
       "0  -1.156460 -0.654724  0.763889   -0.789014   -0.003357        0.305877   \n",
       "1  -1.452829  1.527360 -0.314193   -0.696292   -0.003357        4.312132   \n",
       "2  -1.464683 -0.654724 -0.468204   -0.970563   -0.003357        1.307441   \n",
       "3  -0.563724 -0.654724  0.147842   -0.399582   -0.003357        0.305877   \n",
       "4  -1.156460 -0.654724  1.225924   -0.603570   -0.003357        2.309004   \n",
       "\n",
       "    pub_rec  revol_bal  revol_util  total_acc  initial_list_status  \\\n",
       "0 -0.335522  -0.145932    1.201140  -1.373775             -0.97077   \n",
       "1 -0.335522  -0.679268   -1.912396  -1.796028             -0.97077   \n",
       "2 -0.335522  -0.622684    1.821333  -1.289324             -0.97077   \n",
       "3 -0.335522  -0.504878   -1.426299   0.990842             -0.97077   \n",
       "4 -0.335522  -0.399424   -1.120393  -1.120423             -0.97077   \n",
       "\n",
       "   last_pymnt_amnt  last_credit_pull_d  collections_12_mths_ex_med  \\\n",
       "0        -0.415561            0.302329                   -0.107149   \n",
       "1        -0.426398           -3.540893                   -0.107149   \n",
       "2        -0.315809            0.302329                   -0.107149   \n",
       "3        -0.376798           -1.344122                   -0.107149   \n",
       "4        -0.417770           -0.247992                   -0.107149   \n",
       "\n",
       "   mths_since_last_major_derog  dti_joint  tot_cur_bal  open_acc_6m   il_util  \\\n",
       "0                     0.576897  -0.023957    -0.843347    -0.103891 -0.139209   \n",
       "1                     0.576897  -0.023957    -0.843347    -0.103891 -0.139209   \n",
       "2                     0.576897  -0.023957    -0.843347    -0.103891 -0.139209   \n",
       "3                     0.576897  -0.023957    -0.843347    -0.103891 -0.139209   \n",
       "4                     0.576897  -0.023957    -0.843347    -0.103891 -0.139209   \n",
       "\n",
       "   max_bal_bc  inq_last_12m  emp_type_Accountant  emp_type_Admin  \\\n",
       "0   -0.116273     -0.088306            -0.109205       -0.104843   \n",
       "1   -0.116273     -0.088306            -0.109205       -0.104843   \n",
       "2   -0.116273     -0.088306            -0.109205       -0.104843   \n",
       "3   -0.116273     -0.088306            -0.109205       -0.104843   \n",
       "4   -0.116273     -0.088306            -0.109205       -0.104843   \n",
       "\n",
       "   emp_type_Analyst  emp_type_Assistant  emp_type_Clergy  emp_type_Clerk  \\\n",
       "0         -0.157505           -0.147428        -0.039637       -0.077757   \n",
       "1         -0.157505           -0.147428        -0.039637       -0.077757   \n",
       "2         -0.157505           -0.147428        -0.039637       -0.077757   \n",
       "3         -0.157505           -0.147428        -0.039637       -0.077757   \n",
       "4         -0.157505           -0.147428        -0.039637       -0.077757   \n",
       "\n",
       "   emp_type_Designer  emp_type_Director  emp_type_Education  \\\n",
       "0          -0.052789          -0.196448           -0.186479   \n",
       "1          -0.052789          -0.196448           -0.186479   \n",
       "2          -0.052789          -0.196448           -0.186479   \n",
       "3          -0.052789          -0.196448           -0.186479   \n",
       "4          -0.052789          -0.196448           -0.186479   \n",
       "\n",
       "   emp_type_Executive  emp_type_Healer  emp_type_Manager  emp_type_Operator  \\\n",
       "0           -0.180873        -0.245189         -0.435078          -0.169413   \n",
       "1           -0.180873        -0.245189         -0.435078          -0.169413   \n",
       "2           -0.180873        -0.245189         -0.435078          -0.169413   \n",
       "3           -0.180873        -0.245189         -0.435078          -0.169413   \n",
       "4           -0.180873        -0.245189         -0.435078          -0.169413   \n",
       "\n",
       "   emp_type_Technical  emp_type_Vol  home_ownership_OTHER  \\\n",
       "0           -0.395006     -0.172517             -0.016276   \n",
       "1           -0.395006     -0.172517             -0.016276   \n",
       "2           -0.395006     -0.172517             -0.016276   \n",
       "3           -0.395006     -0.172517             -0.016276   \n",
       "4           -0.395006     -0.172517             -0.016276   \n",
       "\n",
       "   home_ownership_RENT  verification_status_Source Verified  \\\n",
       "0               1.2214                            -0.768632   \n",
       "1               1.2214                             1.301013   \n",
       "2               1.2214                            -0.768632   \n",
       "3               1.2214                             1.301013   \n",
       "4               1.2214                             1.301013   \n",
       "\n",
       "   verification_status_Verified  purpose_credit_card  purpose_major_purchase  \\\n",
       "0                      1.431317             1.817653               -0.140912   \n",
       "1                     -0.698657            -0.550160               -0.140912   \n",
       "2                     -0.698657            -0.550160               -0.140912   \n",
       "3                     -0.698657            -0.550160               -0.140912   \n",
       "4                     -0.698657            -0.550160               -0.140912   \n",
       "\n",
       "   purpose_other  purpose_small_business  purpose_wedding  addr_state_AL  \\\n",
       "0      -0.225373               -0.108777        -0.051496      -0.113061   \n",
       "1      -0.225373               -0.108777        -0.051496      -0.113061   \n",
       "2      -0.225373                9.193151        -0.051496      -0.113061   \n",
       "3       4.437084               -0.108777        -0.051496      -0.113061   \n",
       "4      -0.225373               -0.108777        19.418825      -0.113061   \n",
       "\n",
       "   addr_state_CO  addr_state_DC  addr_state_FL  addr_state_IL  addr_state_KS  \\\n",
       "0      -0.147149      -0.052423      -0.271536      -0.204067      -0.094934   \n",
       "1      -0.147149      -0.052423      -0.271536      -0.204067      -0.094934   \n",
       "2      -0.147149      -0.052423      -0.271536       4.900357      -0.094934   \n",
       "3      -0.147149      -0.052423      -0.271536      -0.204067      -0.094934   \n",
       "4      -0.147149      -0.052423      -0.271536      -0.204067      -0.094934   \n",
       "\n",
       "   addr_state_ME  addr_state_MS  addr_state_ND  addr_state_NE  addr_state_NH  \\\n",
       "0      -0.024331      -0.065744       -0.02324      -0.036428      -0.069732   \n",
       "1      -0.024331      -0.065744       -0.02324      -0.036428      -0.069732   \n",
       "2      -0.024331      -0.065744       -0.02324      -0.036428      -0.069732   \n",
       "3      -0.024331      -0.065744       -0.02324      -0.036428      -0.069732   \n",
       "4      -0.024331      -0.065744       -0.02324      -0.036428      -0.069732   \n",
       "\n",
       "   addr_state_NV  addr_state_NY  addr_state_SC  addr_state_TX  addr_state_VA  \\\n",
       "0      -0.119254      -0.301818      -0.110158      -0.295217      -0.174612   \n",
       "1      -0.119254      -0.301818      -0.110158      -0.295217      -0.174612   \n",
       "2      -0.119254      -0.301818      -0.110158      -0.295217      -0.174612   \n",
       "3      -0.119254      -0.301818      -0.110158      -0.295217      -0.174612   \n",
       "4      -0.119254      -0.301818      -0.110158      -0.295217      -0.174612   \n",
       "\n",
       "       frac  \n",
       "0  1.171905  \n",
       "1  0.403643  \n",
       "2  1.250750  \n",
       "3  1.222458  \n",
       "4  1.125997  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    255720.000000\n",
       "mean          1.014842\n",
       "std           0.297824\n",
       "min           0.000000\n",
       "25%           1.031176\n",
       "50%           1.102847\n",
       "75%           1.175711\n",
       "max           1.763947\n",
       "Name: frac, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['frac'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('frac', axis = 1)\n",
    "y = data['frac']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(255720, 61)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(data = data, x = 'frac');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # Use the GPU\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Use the CPU\n",
    "    print(\"GPU is not available, using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the GPU through the name cuda\n",
    "torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fit function that takes the X-train, y-train, epochs and Batch Size and fits the model\n",
    "def train_model(X_train, y_train, epochs, batch_sizes, net):\n",
    "    y_train = np.array(y_train).reshape(-1,1)\n",
    "\n",
    "    # Create an instance of the model with the correct input_dim\n",
    "    model = net(X_train.shape[1])\n",
    "\n",
    "    # Check if the GPU is available and save it to device. If not use the cpu\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # Move the model over to the device that is available\n",
    "    model.to(device)\n",
    "\n",
    "    # Define the loss function\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Define the optimizer\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    X_train_tensor = torch.Tensor(X_train.values).float().to(device)\n",
    "    y_train_tensor = torch.Tensor(y_train).float().to(device)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = epochs\n",
    "    batch_size = batch_sizes\n",
    "    global losses\n",
    "    losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at the start of each epoch\n",
    "        indices = np.random.permutation(len(X_train_tensor))\n",
    "        shuffled_X = X_train_tensor[indices]\n",
    "        shuffled_y = y_train_tensor[indices]\n",
    "        \n",
    "        # Set the model to training mode\n",
    "        model.train()\n",
    "\n",
    "        # Mini-batch training\n",
    "        for i in range(0, len(X_train_tensor), batch_size):\n",
    "\n",
    "            batch_X = shuffled_X[i:i+batch_size]\n",
    "            batch_y = shuffled_y[i:i+batch_size]\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print loss and accuracy\n",
    "            print(f'\\rEpoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(X_train_tensor)}], Loss: {loss.item():.4f}', end = '\\r')\n",
    "        \n",
    "        losses.append(loss.item()) # type: ignore\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that takes in the loss from the train_model as an argument\n",
    "def loss_plot(loss, param_k = False, ticks_k = False):\n",
    "    if ticks_k == False:\n",
    "        # Create a plot of the the epics and the losses for each epoch\n",
    "        plt.plot(range(1,len(loss) + 1), loss);\n",
    "        # Match the ticks to the epochs\n",
    "        plt.xticks(np.arange(1, len(loss) + 1));\n",
    "    else:\n",
    "        # Create a plot of the the epics and the losses for each epoch\n",
    "        plt.plot(range(ticks_k[0], ticks_k[1]), loss); # type: ignore\n",
    "        # Match the ticks to the epochs\n",
    "        plt.xticks(np.arange(ticks_k[0], ticks_k[1] + 1)); # type: ignore\n",
    "    if param_k == False:\n",
    "        # Label the x-axis of the graph\n",
    "        plt.xlabel(\"Epoch #\");\n",
    "    else:\n",
    "        # Label the x-axis of the graph\n",
    "        plt.xlabel(\"K - Value\");\n",
    "    # Label the y-axis of the graph\n",
    "    plt.ylabel(\"Cost\");\n",
    "    # Title the graph\n",
    "    plt.title(\"Model Cost\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to evaluate the model\n",
    "def eval_model(X_test, y_test, model):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Move the model and data back to the CPU\n",
    "    model.to(\"cpu\")\n",
    "    X_test_tensor = torch.Tensor(X_test.values).float()\n",
    "\n",
    "    # Make Preds a global variable for the roc graph\n",
    "    global preds\n",
    "    # Perform predictions on the test set\n",
    "    with torch.no_grad():\n",
    "        preds = model(X_test_tensor)\n",
    "\n",
    "    # Convert the predictions tensor to a numpy array\n",
    "    preds = preds.numpy()\n",
    "\n",
    "    # Calculate the rootmean square\n",
    "    plt.figure(figsize = (20,8))\n",
    "    plt.scatter(range(len(preds)), preds - y_test, s = .01, c = 'red')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model class Net from the parent class nn.Module\n",
    "class Net(nn.Module):\n",
    "    # Initialize the class with \n",
    "    def __init__(self, input_dim):\n",
    "        # Initialize nn.Module with super\n",
    "        super(Net, self).__init__()\n",
    "        # Create the first layer of the neural net\n",
    "        self.fc1 = nn.Linear(input_dim, 61)\n",
    "        # Create the middle layers of the network with 12 nodes that connect to 12 nodes\n",
    "        self.fc2 = nn.Linear(61, 61)\n",
    "        self.fc3 = nn.Linear(61, 61)\n",
    "        self.fc4 = nn.Linear(61, 61)\n",
    "        self.fc5 = nn.Linear(61, 61)\n",
    "        self.fc6 = nn.Linear(61, 61)\n",
    "        self.fc7 = nn.Linear(61, 61)\n",
    "        self.fc8 = nn.Linear(61, 53)\n",
    "        self.fc9 = nn.Linear(53, 38)\n",
    "        self.fc10 = nn.Linear(38, 25)\n",
    "        self.fc11 = nn.Linear(25, 14)\n",
    "        # Create a layer that has 12 nodes that connect to 8 nodes\n",
    "        self.fc12 = nn.Linear(14, 8)\n",
    "        # Create the last layer that takes 8 nodes and compresses it down to 1\n",
    "        self.fc13 = nn.Linear(8, 1)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    # Define a method for the forward propagation of the model\n",
    "    def forward(self, x):\n",
    "        # Create the connections \n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.activation(self.fc4(x))\n",
    "        x = self.activation(self.fc5(x))\n",
    "        x = self.activation(self.fc6(x))\n",
    "        x = self.activation(self.fc7(x))\n",
    "        x = self.activation(self.fc8(x))\n",
    "        x = self.activation(self.fc9(x))\n",
    "        x = self.activation(self.fc10(x))\n",
    "        x = self.activation(self.fc11(x))\n",
    "        x = self.activation(self.fc12(x))\n",
    "        x = self.activation(self.fc13(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/38], Step [191001/191790], Loss: 0.0271\r"
     ]
    }
   ],
   "source": [
    "bignet = train_model(X_train, y_train, epochs = 38, batch_sizes = 1000, net = Net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m eval_model(X_test, y_test, bignet)\n",
      "Cell \u001b[1;32mIn[60], line 21\u001b[0m, in \u001b[0;36meval_model\u001b[1;34m(X_test, y_test, model)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39m# Calculate the rootmean square\u001b[39;00m\n\u001b[0;32m     20\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize \u001b[39m=\u001b[39m (\u001b[39m20\u001b[39m,\u001b[39m8\u001b[39m))\n\u001b[1;32m---> 21\u001b[0m plt\u001b[39m.\u001b[39mscatter(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(preds)), preds \u001b[39m-\u001b[39;49m y_test, s \u001b[39m=\u001b[39m \u001b[39m.01\u001b[39m, c \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mred\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jdrel\\miniconda3\\envs\\Capstone-2\\lib\\site-packages\\pandas\\core\\generic.py:2113\u001b[0m, in \u001b[0;36mNDFrame.__array_ufunc__\u001b[1;34m(self, ufunc, method, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2109\u001b[0m \u001b[39m@final\u001b[39m\n\u001b[0;32m   2110\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array_ufunc__\u001b[39m(\n\u001b[0;32m   2111\u001b[0m     \u001b[39mself\u001b[39m, ufunc: np\u001b[39m.\u001b[39mufunc, method: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39minputs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any\n\u001b[0;32m   2112\u001b[0m ):\n\u001b[1;32m-> 2113\u001b[0m     \u001b[39mreturn\u001b[39;00m arraylike\u001b[39m.\u001b[39marray_ufunc(\u001b[39mself\u001b[39m, ufunc, method, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jdrel\\miniconda3\\envs\\Capstone-2\\lib\\site-packages\\pandas\\core\\arraylike.py:265\u001b[0m, in \u001b[0;36marray_ufunc\u001b[1;34m(self, ufunc, method, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[0;32m    264\u001b[0m \u001b[39m# for binary ops, use our custom dunder methods\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m result \u001b[39m=\u001b[39m maybe_dispatch_ufunc_to_dunder_op(\u001b[39mself\u001b[39m, ufunc, method, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    266\u001b[0m \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[0;32m    267\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\jdrel\\miniconda3\\envs\\Capstone-2\\lib\\site-packages\\pandas\\_libs\\ops_dispatch.pyx:113\u001b[0m, in \u001b[0;36mpandas._libs.ops_dispatch.maybe_dispatch_ufunc_to_dunder_op\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\jdrel\\miniconda3\\envs\\Capstone-2\\lib\\site-packages\\pandas\\core\\ops\\common.py:72\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[0;32m     70\u001b[0m other \u001b[39m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 72\u001b[0m \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39;49m, other)\n",
      "File \u001b[1;32mc:\\Users\\jdrel\\miniconda3\\envs\\Capstone-2\\lib\\site-packages\\pandas\\core\\arraylike.py:114\u001b[0m, in \u001b[0;36mOpsMixin.__rsub__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@unpack_zerodim_and_defer\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m__rsub__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__rsub__\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_arith_method(other, roperator\u001b[39m.\u001b[39;49mrsub)\n",
      "File \u001b[1;32mc:\\Users\\jdrel\\miniconda3\\envs\\Capstone-2\\lib\\site-packages\\pandas\\core\\series.py:6259\u001b[0m, in \u001b[0;36mSeries._arith_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   6257\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_arith_method\u001b[39m(\u001b[39mself\u001b[39m, other, op):\n\u001b[0;32m   6258\u001b[0m     \u001b[39mself\u001b[39m, other \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39malign_method_SERIES(\u001b[39mself\u001b[39m, other)\n\u001b[1;32m-> 6259\u001b[0m     \u001b[39mreturn\u001b[39;00m base\u001b[39m.\u001b[39;49mIndexOpsMixin\u001b[39m.\u001b[39;49m_arith_method(\u001b[39mself\u001b[39;49m, other, op)\n",
      "File \u001b[1;32mc:\\Users\\jdrel\\miniconda3\\envs\\Capstone-2\\lib\\site-packages\\pandas\\core\\base.py:1325\u001b[0m, in \u001b[0;36mIndexOpsMixin._arith_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   1322\u001b[0m rvalues \u001b[39m=\u001b[39m ensure_wrapped_if_datetimelike(rvalues)\n\u001b[0;32m   1324\u001b[0m \u001b[39mwith\u001b[39;00m np\u001b[39m.\u001b[39merrstate(\u001b[39mall\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> 1325\u001b[0m     result \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39;49marithmetic_op(lvalues, rvalues, op)\n\u001b[0;32m   1327\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_construct_result(result, name\u001b[39m=\u001b[39mres_name)\n",
      "File \u001b[1;32mc:\\Users\\jdrel\\miniconda3\\envs\\Capstone-2\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:226\u001b[0m, in \u001b[0;36marithmetic_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    222\u001b[0m     _bool_arith_check(op, left, right)\n\u001b[0;32m    224\u001b[0m     \u001b[39m# error: Argument 1 to \"_na_arithmetic_op\" has incompatible type\u001b[39;00m\n\u001b[0;32m    225\u001b[0m     \u001b[39m# \"Union[ExtensionArray, ndarray[Any, Any]]\"; expected \"ndarray[Any, Any]\"\u001b[39;00m\n\u001b[1;32m--> 226\u001b[0m     res_values \u001b[39m=\u001b[39m _na_arithmetic_op(left, right, op)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[39mreturn\u001b[39;00m res_values\n",
      "File \u001b[1;32mc:\\Users\\jdrel\\miniconda3\\envs\\Capstone-2\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:165\u001b[0m, in \u001b[0;36m_na_arithmetic_op\u001b[1;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[0;32m    162\u001b[0m     func \u001b[39m=\u001b[39m partial(expressions\u001b[39m.\u001b[39mevaluate, op)\n\u001b[0;32m    164\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 165\u001b[0m     result \u001b[39m=\u001b[39m func(left, right)\n\u001b[0;32m    166\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    167\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_cmp \u001b[39mand\u001b[39;00m (is_object_dtype(left\u001b[39m.\u001b[39mdtype) \u001b[39mor\u001b[39;00m is_object_dtype(right)):\n\u001b[0;32m    168\u001b[0m         \u001b[39m# For object dtype, fallback to a masked operation (only operating\u001b[39;00m\n\u001b[0;32m    169\u001b[0m         \u001b[39m#  on the non-missing values)\u001b[39;00m\n\u001b[0;32m    170\u001b[0m         \u001b[39m# Don't do this for comparisons, as that will handle complex numbers\u001b[39;00m\n\u001b[0;32m    171\u001b[0m         \u001b[39m#  incorrectly, see GH#32047\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jdrel\\miniconda3\\envs\\Capstone-2\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:241\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(op, a, b, use_numexpr)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[39mif\u001b[39;00m op_str \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    239\u001b[0m     \u001b[39mif\u001b[39;00m use_numexpr:\n\u001b[0;32m    240\u001b[0m         \u001b[39m# error: \"None\" not callable\u001b[39;00m\n\u001b[1;32m--> 241\u001b[0m         \u001b[39mreturn\u001b[39;00m _evaluate(op, op_str, a, b)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    242\u001b[0m \u001b[39mreturn\u001b[39;00m _evaluate_standard(op, op_str, a, b)\n",
      "File \u001b[1;32mc:\\Users\\jdrel\\miniconda3\\envs\\Capstone-2\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:129\u001b[0m, in \u001b[0;36m_evaluate_numexpr\u001b[1;34m(op, op_str, a, b)\u001b[0m\n\u001b[0;32m    126\u001b[0m     _store_test_result(result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    128\u001b[0m \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 129\u001b[0m     result \u001b[39m=\u001b[39m _evaluate_standard(op, op_str, a, b)\n\u001b[0;32m    131\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\jdrel\\miniconda3\\envs\\Capstone-2\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:70\u001b[0m, in \u001b[0;36m_evaluate_standard\u001b[1;34m(op, op_str, a, b)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[39mif\u001b[39;00m _TEST_MODE:\n\u001b[0;32m     69\u001b[0m     _store_test_result(\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m---> 70\u001b[0m \u001b[39mreturn\u001b[39;00m op(a, b)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_model(X_test, y_test, bignet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a random forest model\n",
    "rf = RandomForestClassifier(n_jobs = 4)\n",
    "# Create a set of values to search\n",
    "param_grid = {'n_estimators': [5, 50, 100, 150, 200, 300, 600]}  \n",
    "# Create the grid search object with 5 cross validational folds\n",
    "grid_search = GridSearchCV(rf, param_grid, cv = 2, verbose = 1)  \n",
    "# Fit the data to the grid search\n",
    "grid_search.fit(X, y_train)\n",
    "# Save the best parameters  \n",
    "best_params = grid_search.best_params_\n",
    "# Print the best parameters\n",
    "best_params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Capstone-3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
